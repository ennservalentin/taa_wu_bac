{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02 Backtesting Environment\n",
    "---\n",
    "This notebook executes the first notebook and then provides methods for testing a set on input weights on a selected timeframe\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'config' from '/Users/valentinennser/Desktop/taa_bac/config.py'>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%run 01_data_loading.ipynb\n",
    "import importlib\n",
    "import config\n",
    "from datetime import datetime, timedelta\n",
    "import scipy.optimize as opt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import minimize\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "importlib.reload(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backtest_portfolio(weights, start_date, end_date):\n",
    "\n",
    "    \"\"\"\n",
    "    This function calculates the cumulative returns of\n",
    "    the portfolio according to the weights provided.\n",
    "\n",
    "    Order of weights:\n",
    "    SPY, FI, REIT, GOLD\n",
    "\n",
    "    example:\n",
    "    backtest_portfolio([0.2,0.2,0.3,0.3], '2010-01-01', '2019-12-31')\n",
    "    \"\"\"\n",
    "\n",
    "    #check weights dimensions\n",
    "    if len(weights) != 4:\n",
    "        raise ValueError(\"Weights must be a list of 4 elements\")\n",
    "    elif sum(weights) != 1:\n",
    "        raise ValueError(\"Weights must sum to 1\")\n",
    "    \n",
    "    #create a new dataframe with all prices\n",
    "    SPY_prices = get_data(config.SPY,start_date,end_date)\n",
    "    FI_prices = get_data(config.FIXED_INCOME,start_date,end_date)\n",
    "    REIT_prices = get_data(config.REIT,start_date,end_date)\n",
    "    GOLD_prices = get_data(config.GOLD,start_date,end_date)\n",
    "\n",
    "    data_all = pd.concat([SPY_prices, FI_prices, REIT_prices, GOLD_prices], axis=1)\n",
    "    \n",
    "    #create a new dataframe with all returns\n",
    "    data_all_returns = data_all.ffill().pct_change().dropna()\n",
    "    \n",
    "    #weigh them according to the weights\n",
    "    for i in range(len(weights)):\n",
    "        data_all_returns.iloc[:,i] = data_all_returns.iloc[:,i]*weights[i]\n",
    "    \n",
    "    #calculate the portfolio returns\n",
    "    data_all_returns['portfolio_returns'] = data_all_returns.sum(axis=1)\n",
    "\n",
    "    #calculate the portfolio cumulative returns\n",
    "    data_all_returns['cumulative_returns'] = (1 + data_all_returns['portfolio_returns']).cumprod()\n",
    "\n",
    "    return data_all_returns['cumulative_returns'].iloc[-1] - 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def portfolio_vol(weights, covmat):\n",
    "    \"\"\"\n",
    "    helper function to calculate portfolio volatility\n",
    "    \"\"\"\n",
    "    return (weights.T @ covmat @ weights)**0.5\n",
    "\n",
    "def portfolio_return(weights,returns):\n",
    "    \"\"\"\n",
    "    helper function to calculate portfolio return\n",
    "    \"\"\"\n",
    "    return weights.T @ returns\n",
    "\n",
    "def annualize_retss(r, periods_per_year):\n",
    "    \"\"\"\n",
    "    helper function to annualize returns\n",
    "    \"\"\"\n",
    "    compounded_growth = (1 + r).prod()\n",
    "    n_periods = r.shape[0]\n",
    "    return compounded_growth**(periods_per_year / n_periods) - 1\n",
    "\n",
    "def max_profit(df_prices, window_size=30, min_weight=0.1):\n",
    "    weights_to_return = pd.DataFrame()\n",
    "\n",
    "    for i in range(len(df_prices) - window_size + 1):\n",
    "        df_prices_window = df_prices.iloc[i:i+window_size]\n",
    "        df_returns = df_prices_window.pct_change(fill_method=None).dropna()\n",
    "\n",
    "        expected_returns = annualize_retss(df_returns, 252/window_size)\n",
    "\n",
    "        n = expected_returns.shape[0]\n",
    "\n",
    "        # Find the index of the asset with the highest expected return\n",
    "        max_return_index = np.argmax(expected_returns)\n",
    "\n",
    "        # Set weights\n",
    "        weights = np.full(n, min_weight)\n",
    "        weights[max_return_index] = 1.0 - (n-1) * min_weight\n",
    "\n",
    "        new_weights = pd.DataFrame(weights).T\n",
    "        weights_to_return = pd.concat([weights_to_return, new_weights], axis=0)\n",
    "\n",
    "    weights_to_return.index = df_prices.iloc[window_size-1:].index\n",
    "    weights_to_return.columns = df_prices.columns\n",
    "    return weights_to_return\n",
    "\n",
    "def gmv(df_prices, rf_rate, window_size = 30):\n",
    "    \"\"\"\n",
    "    Returns the weights of the Global Minimum Volatility portfolio given a covariance matrix\n",
    "    \"\"\"\n",
    "    # I love this function, what we are doing here is we are feeding the MSR optimizer the idea that all the returns are the same and the way it reacts\n",
    "    # is it starts to increase the SR by lowering volatility and does so until it reaches the point of lowest possible volatility\n",
    "\n",
    "    for i in range(len(df_prices) - window_size + 1):\n",
    "        df_prices_window = df_prices.iloc[i:i+window_size]\n",
    "        df_returns = df_prices_window.pct_change(fill_method=None).dropna()\n",
    "\n",
    "        cov_matrix = df_returns.cov() * 252/window_size\n",
    "        n = cov_matrix.shape[0]\n",
    "\n",
    "        weights = msr(rf_rate, np.repeat(1, n), cov_matrix)\n",
    "       \n",
    "        new_weights = pd.DataFrame(weights.x).T\n",
    "        weights_to_return = pd.concat([weights_to_return, new_weights], axis=0)\n",
    "\n",
    "    weights_to_return.index = df_prices.iloc[window_size-1:].index\n",
    "    weights_to_return.columns = df_prices.columns\n",
    "        \n",
    "    return weights_to_return\n",
    "\n",
    "\n",
    "def msr(df_prices,rf_rate=0.01, window_size = 30, min_weight = 0.1):\n",
    "    \"\"\"\n",
    "    Returns the weights of the portfolio that gives you the maximum sharpe ratio\n",
    "    given the riskfree rate and expected returns and a covariance matrix\n",
    "\n",
    "    example: msr(expected_returns, covariance_matrix, rf_rate)\n",
    "    \"\"\"\n",
    "    weights_to_return = pd.DataFrame()\n",
    "    \n",
    "    for i in range(len(df_prices) - window_size + 1):\n",
    "        df_prices_window = df_prices.iloc[i:i+window_size]\n",
    "        df_returns = df_prices_window.pct_change(fill_method=None).dropna()\n",
    "\n",
    "\n",
    "        expected_returns  = annualize_rets(df_returns, 252/window_size)\n",
    "        cov_matrix = df_returns.cov() * 252/window_size\n",
    "\n",
    "        n = expected_returns.shape[0] \n",
    "        init_guess = np.repeat(1/n, n) \n",
    "        bounds = ((min_weight, 1.0),) * n \n",
    "\n",
    "        weights_sum_to_1 = {'type': 'eq',\n",
    "                        'fun': lambda weights: np.sum(weights) - 1}\n",
    "        #maximize function does not exist, thus what we will be doing is minimizing the negative sharpe ratio\n",
    "        def neg_sharpe(weights, riskfree_rate, er, cov):\n",
    "            \"\"\"\n",
    "            Returns the negative of the sharpe ratio\n",
    "            of the given portfolio\n",
    "            \"\"\"\n",
    "            r = portfolio_return(weights, er)\n",
    "            vol = portfolio_vol(weights, cov)\n",
    "            return -(r - riskfree_rate)/vol\n",
    "\n",
    "        weights = opt.minimize(neg_sharpe, init_guess,\n",
    "                        args=(rf_rate, expected_returns, cov_matrix), method='SLSQP',\n",
    "                        options={'disp': False},\n",
    "                        constraints=(weights_sum_to_1,),\n",
    "                        bounds=bounds)\n",
    "        \n",
    "        new_weights = pd.DataFrame(weights.x).T\n",
    "        weights_to_return = pd.concat([weights_to_return, new_weights], axis=0)\n",
    "\n",
    "    weights_to_return.index = df_prices.iloc[window_size-1:].index\n",
    "    \n",
    "    weights_to_return.columns = df_prices.columns\n",
    "        \n",
    "    return weights_to_return\n",
    "\n",
    "\n",
    "\n",
    "def annualize_rets(r, periods_per_year):\n",
    "    \"\"\"\n",
    "    Annualizes a set of returns given the periods per year\n",
    "    \"\"\"\n",
    "    compounded_growth = (1+r).prod() \n",
    "    n_periods = r.shape[0] \n",
    "    return compounded_growth**(periods_per_year/n_periods)-1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_trailing_returns(df, window_size):\n",
    "    \"\"\"\n",
    "    This function calculates the trailing returns of a given dataframe\n",
    "    \"\"\"\n",
    "    returns = df.pct_change()\n",
    "    trailing_returns = returns.rolling(window=window_size).sum()  # Calculate trailing returns\n",
    "    return trailing_returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_trailing_risk_parity_weights(prices, window_size=30):\n",
    "    \"\"\"\n",
    "    This function calculates the risk parity weights using a rolling window\n",
    "    \"\"\"\n",
    "    # Calculate daily returns\n",
    "    returns = prices.pct_change().dropna()\n",
    "\n",
    "    #remove column \"cash\" by colname\n",
    "    returns = returns.drop(columns=['cash'])\n",
    "\n",
    "    # Calculate Risk Parity Weights using a rolling window\n",
    "    risk_parity_weights = returns.rolling(window=window_size).apply(lambda x: 1 / (x.std()), raw=True)\n",
    "    risk_parity_weights = risk_parity_weights.div(risk_parity_weights.sum(axis=1), axis=0)\n",
    "\n",
    "    risk_parity_weights[\"cash\"] = 0\n",
    "    \n",
    "    return risk_parity_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_and_test_data(start_date, length, portfolio = config.MAX_SHARPE, window_size = 30, min_weight = 0.1,rf_rate = 0.02, freq = \"M\"):\n",
    "\n",
    "    \"\"\"\n",
    "    Creates a training and test set from the available data, given a start date \n",
    "    and a length. The output sets contain the set of 11 macroeconomic input variables\n",
    "    for each day as well as the optimal 30 day forward calculated risk parity portfolio\n",
    "    weights. \n",
    "\n",
    "    example: get_training_and_test_data(\"2012-01-01\",365)\n",
    "    \"\"\"\n",
    "    print(\"Getting data...\")\n",
    "    #calculating the end_date\n",
    "    temp_train_start_date_obj = datetime.strptime(start_date, \"%Y-%m-%d\")\n",
    "    #-30 days cause of 30 day lag (NAs) when calculating 30day rolling returns later\n",
    "    train_start_date_obj =  temp_train_start_date_obj - timedelta(days=30)\n",
    "    train_start_date_str = train_start_date_obj.strftime(\"%Y-%m-%d\")\n",
    "    train_end_date_obj =  train_start_date_obj + timedelta(days=length+30)\n",
    "    train_end_date_str = train_end_date_obj.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    test_start_date_obj = train_end_date_obj + timedelta(days=2)\n",
    "    test_start_date_str = train_end_date_obj.strftime(\"%Y-%m-%d\")\n",
    "    test_end_date_obj = test_start_date_obj + timedelta(days = length-1)\n",
    "    test_end_date_str = test_end_date_obj.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    #pulling macro indicators in the select timeframe for train & test\n",
    "    macro_indicators_train = get_indicators(train_start_date_str, train_end_date_str).dropna()\n",
    "    macro_indicators_test = get_indicators(test_start_date_str, test_end_date_str).dropna()\n",
    "\n",
    "    #pulling index prices in the select timeframe\n",
    "    df_prices_train = get_indices(train_start_date_str, train_end_date_str)\n",
    "    df_prices_test = get_indices(test_start_date_str, test_end_date_str)\n",
    "\n",
    "    #getting optimal portfolio weights\n",
    "    if(portfolio == config.RISK_PARITY):\n",
    "        train_weights = calculate_trailing_risk_parity_weights(df_prices_train)\n",
    "        test_weights = calculate_trailing_risk_parity_weights(df_prices_test)\n",
    "\n",
    "    elif(portfolio == config.MAX_SHARPE):\n",
    "        train_weights = msr(df_prices_train,rf_rate,int(window_size),min_weight)\n",
    "        test_weights = msr(df_prices_test,rf_rate,int(window_size),min_weight)\n",
    "\n",
    "    elif(portfolio == config.EQUAL):\n",
    "        train_data = np.full((len(df_prices_train), len(df_prices_train.columns)), 0.2)\n",
    "        test_data = np.full((len(df_prices_test), len(df_prices_test.columns)), 0.2)\n",
    "        train_weights = pd.DataFrame(train_data,columns = df_prices_train.columns,index = df_prices_train.index)\n",
    "        test_weights = pd.DataFrame(test_data,columns = df_prices_test.columns,index = df_prices_test.index)\n",
    "\n",
    "    elif(portfolio == config.MAX_PROFIT):\n",
    "        train_weights = max_profit(df_prices_train,window_size,min_weight)\n",
    "        test_weights = max_profit(df_prices_test,window_size,min_weight)\n",
    "\n",
    "    elif(portfolio == config.GMV):\n",
    "        train_weights = gmv(df_prices_train,rf_rate,window_size)\n",
    "        test_weights = gmv(df_prices_test,rf_rate,window_size)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Portfolio must be either risk parity, max sharpe or equal\")\n",
    "\n",
    "    macro_indicators_train = pd.concat([macro_indicators_train,train_weights], axis = 1).dropna()\n",
    "    macro_indicators_test = pd.concat([macro_indicators_test,test_weights], axis = 1).dropna()\n",
    "\n",
    "    macro_indicators_train = macro_indicators_train.resample(freq).last(\"D\")\n",
    "    macro_indicators_test = macro_indicators_test.resample(freq).last(\"D\")\n",
    "    \n",
    "    return macro_indicators_train,macro_indicators_test\n",
    "\n",
    "\n",
    "def linear_regression(train_set, test_set):\n",
    "    print(\"Running linear regression...\")\n",
    "    macroeconomic_indicators = [\n",
    "    \"CONSSENT_Consumer_Confidence\", \"PIDSDPS_SAVINGS_DISP_INC\", \"USYC2Y10_10y_2y\",\n",
    "    \"CPI_YOY_US_CPI\", \"CSI_BARC_HY_SPREAD\", \"GDP_CURY_US_GDP_GROWTH\",\n",
    "    \"S&P500_PE_PE_Ratio_S&P\", \"LB1_Lumber\", \"CL1_OIL_WTI\", \"GLDHG_COPPER_GOLD_RATIO\",\n",
    "    \"USURTOT_US_UNEMPLOYMENT\"\n",
    "    ]\n",
    "    weight_predictions = [\"LUATTRUU_Fixed_Income\", \"RMZ_REITs\", \"SPX_Equities\", \"XAU_Gold\", \"cash\"]\n",
    "\n",
    "    # Drop rows with NaNs or Infs\n",
    "    train_set_clean = train_set.replace([np.inf, -np.inf], np.nan).dropna(subset=macroeconomic_indicators + weight_predictions)\n",
    "    test_set_clean = test_set.replace([np.inf, -np.inf], np.nan).dropna(subset=macroeconomic_indicators)\n",
    "\n",
    "    macroeconomic_indicators_to_train = train_set_clean[macroeconomic_indicators]\n",
    "    outcome_weights_to_train = train_set_clean[weight_predictions]\n",
    "\n",
    "    # Instantiate and fit the model\n",
    "    model = MultiOutputRegressor(LinearRegression())\n",
    "    model.fit(macroeconomic_indicators_to_train, outcome_weights_to_train)\n",
    "\n",
    "    # Predict (on training data or separate test data)\n",
    "    weights_train_set_raw = model.predict(macroeconomic_indicators_to_train)  \n",
    "    weights_test_set_raw = model.predict(test_set_clean[macroeconomic_indicators])\n",
    "\n",
    "    weights_train_set = pd.DataFrame(weights_train_set_raw, columns=weight_predictions, index=train_set_clean.index)\n",
    "    weights_test_set = pd.DataFrame(weights_test_set_raw, columns=weight_predictions, index=test_set_clean.index)\n",
    "\n",
    "    return weights_train_set, weights_test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_weights(weights):\n",
    "    \"\"\"\n",
    "    Plots the weights of the portfolio over time\n",
    "\n",
    "    draw_weights(weights)\n",
    "    \"\"\"\n",
    "    #plot weights and show legend\n",
    "    plt.plot(weights)\n",
    "    #plot legend outside\n",
    "    plt.legend(weights.columns,loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "    \n",
    "\n",
    "def draw_returns(weights, portfolio):\n",
    "    \"\"\"\n",
    "    Plots the returns of the portfolio over time\n",
    "\n",
    "    draw_returns(returns)\n",
    "    \"\"\"\n",
    "\n",
    "    #extract date range from weights and get asset prices for that timeframe, -1 row for dropping na\n",
    "    date_range = pd.date_range(start=weights.index.min(), end=weights.index.max(), freq='D')\n",
    "    start_date = date_range[0] - timedelta(days=1)\n",
    "    df_prices = get_indices(start_date.strftime(\"%Y-%m-%d\"), date_range[-1].strftime(\"%Y-%m-%d\")).pct_change().dropna()\n",
    "\n",
    "    #calculate and plot returns\n",
    "    portfolio_return = (weights * df_prices).sum(axis=1)\n",
    "    portfolio_return.cumsum().plot(label = \"NN Weighted Portfolio\")\n",
    "\n",
    "    if portfolio == config.SPY:\n",
    "        #calculate and plot returns\n",
    "        sp500_return = df_prices[config.SPY]\n",
    "        sp500_return.cumsum().plot(label = \"S&P500\")\n",
    "        plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "    elif portfolio == config.EQUAL:\n",
    "        #calculate and plot returns\n",
    "        equal_return = df_prices.mean(axis=1)\n",
    "        equal_return.cumsum().plot(label = \"Equally Weighted Portfolio\")\n",
    "        plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "\n",
    "def draw_strategies(weights_1, portfolio_name_1, weights_2, portfolio_name_2, title, comparison = False):\n",
    "    \"\"\"\n",
    "    Plots the performance of two portfolios over time\n",
    "\n",
    "    draw_strategies(weights_1, weights_2)\n",
    "    \"\"\"\n",
    "    date_range = pd.date_range(start=weights_1.index.min(), end=weights_1.index.max(), freq='D')\n",
    "    start_date = date_range[0] - timedelta(days=1)\n",
    "    df_prices = get_indices(start_date.strftime(\"%Y-%m-%d\"), date_range[-1].strftime(\"%Y-%m-%d\")).pct_change().dropna()\n",
    "\n",
    "    #calculate and plot returns\n",
    "    portfolio_1_return = (weights_1 * df_prices).sum(axis=1)\n",
    "    portfolio_1_return.cumsum().plot(label = portfolio_name_1)\n",
    "\n",
    "    portfolio_2_return = (weights_2 * df_prices).sum(axis=1)\n",
    "    portfolio_2_return.cumsum().plot(label = portfolio_name_2)\n",
    "\n",
    "    portfolio_1_cum = portfolio_1_return.cumsum()\n",
    "    portfolio_2_cum = portfolio_2_return.cumsum()\n",
    "\n",
    "    plt.text(portfolio_1_cum.index[-1], portfolio_1_cum.iloc[-1], \n",
    "             f'{100*portfolio_1_cum.iloc[-1]:.2f}:{\"%\"}', \n",
    "             va='center', ha='left', fontsize=9)\n",
    "\n",
    "    plt.text(portfolio_2_cum.index[-1], portfolio_2_cum.iloc[-1], \n",
    "             f'{100*portfolio_2_cum.iloc[-1]:.2f}:{\"%\"}', \n",
    "             va='center', ha='left', fontsize=9)\n",
    "    \n",
    "    # Plot S&P 500 for comparison\n",
    "    if comparison:\n",
    "        equal_return = df_prices.mean(axis=1)\n",
    "        equal_return.cumsum().plot(label = \"Equal Weights Portfolio\", color='grey')\n",
    "        sp_return_cum = equal_return.cumsum()\n",
    "        plt.text(equal_return.index[-1], sp_return_cum.iloc[-1], \n",
    "             f'{100*sp_return_cum.iloc[-1]:.2f}:{\"%\"}', \n",
    "             va='center', ha='left', fontsize=9)\n",
    "\n",
    "\n",
    "    plt.legend(loc='best', bbox_to_anchor=(1, 0.5))\n",
    "    plt.title(title)\n",
    "\n",
    "\n",
    "\n",
    "def calc_returns(weights, returns):\n",
    "    \"\"\"\n",
    "    Calculates the returns of the portfolio given the weights and the returns\n",
    "\n",
    "    calc_returns(weights, returns)\n",
    "    \"\"\"\n",
    "\n",
    "    portfolio_df = weights.T @ returns\n",
    "    portfolio_df['portfolio_returns'] = portfolio_df.sum(axis=1)\n",
    "    return portfolio_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
